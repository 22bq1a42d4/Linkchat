{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe93748",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers datasets sentencepiece accelerate evaluate rouge_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a8b4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O data.json https://raw.githubusercontent.com/22bq1a42d4/Linkchat/main/data.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc2d4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"data.json\")[\"train\"]\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "print(dataset[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d020ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, sentencepiece as spm\n",
    "\n",
    "texts = []\n",
    "for ex in dataset[\"train\"]:\n",
    "    texts.append(ex[\"question\"])\n",
    "    texts.append(ex[\"answer\"])\n",
    "\n",
    "with open(\"all_texts.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for t in texts:\n",
    "        f.write(t.replace(\"\\n\",\" \") + \"\\n\")\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input='all_texts.txt',\n",
    "    model_prefix='spm_tokenizer',\n",
    "    vocab_size=8000,\n",
    "    model_type='bpe',\n",
    "    character_coverage=1.0\n",
    ")\n",
    "\n",
    "from transformers import T5TokenizerFast\n",
    "tokenizer = T5TokenizerFast(tokenizer_file=\"spm_tokenizer.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fae9b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Config, T5ForConditionalGeneration\n",
    "\n",
    "config = T5Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=256,\n",
    "    d_ff=1024,\n",
    "    num_layers=4,\n",
    "    num_decoder_layers=4,\n",
    "    num_heads=4,\n",
    "    dropout_rate=0.1,\n",
    ")\n",
    "model = T5ForConditionalGeneration(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2794bca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 64\n",
    "max_target_length = 64\n",
    "\n",
    "def preprocess(examples):\n",
    "    inputs = [q.strip() for q in examples[\"question\"]]\n",
    "    targets = [a.strip() for a in examples[\"answer\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "    labels = tokenizer(text_target=targets, max_length=max_target_length, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized = dataset.map(preprocess, batched=True, remove_columns=dataset[\"train\"].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783532ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import evaluate, numpy as np\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {k: round(v,4) for k,v in result.items()}\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=5e-4,\n",
    "    save_total_limit=2,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7028a4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "print(pipe(\"How can I improve my LinkedIn profile?\", max_length=64)[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61366424",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"qa_model\")\n",
    "tokenizer.save_pretrained(\"qa_tokenizer\")\n",
    "\n",
    "from google.colab import files\n",
    "files.download(\"qa_model/config.json\")\n",
    "files.download(\"qa_tokenizer/tokenizer.json\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}